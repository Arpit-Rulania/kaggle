{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f0e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
      "/home/arpitsinghrulania/kaggle/working/yolov5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/home/arpitsinghrulania/kaggle/working\n",
      "Setup complete. Using torch 1.13.0+cu117 (GeForce RTX 2080 Ti)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt  \n",
    "%cd ../\n",
    "import torch\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cf475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (61.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "import torch\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc68f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# got some omp:error 15\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import glob\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# from engine import train_one_epoch, evaluate\n",
    "# import utils\n",
    "\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "IMG_SIZE=1280\n",
    "Selected_Fold=4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c78dbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f568bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arpitsinghrulania/kaggle\n"
     ]
    }
   ],
   "source": [
    "# Get absolute path of root/parent directory\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "print(ROOT_DIR)\n",
    "TRAIN_PATH = F'{ROOT_DIR}/train'\n",
    "TEST_PATH = F'{ROOT_DIR}/test'\n",
    "os.makedirs('COTS/images/train', exist_ok=True)\n",
    "os.makedirs('COTS/images/valid', exist_ok=True)\n",
    "os.makedirs('COTS/labels/train', exist_ok=True)\n",
    "os.makedirs('COTS/labels/valid', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f79210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annotations'] = df['annotations'].apply(eval)\n",
    "df['old_image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e73001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with no bbox: 79.06897578826432%, Images with bbox: 20.93102421173567%\n"
     ]
    }
   ],
   "source": [
    "df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\n",
    "bbox_proportion = (df.num_bbox>0).value_counts(normalize=True) * 100\n",
    "print(\"Images with no bbox: {}%, Images with bbox: {}%\".format(bbox_proportion[0], bbox_proportion[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125988cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.num_bbox > 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40e5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7995006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bboxes'] = df.annotations.apply(get_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86649c1b",
   "metadata": {},
   "source": [
    "## split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf221979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df.sample(frac = 0.8, random_state=8)\n",
    "# stratified sampling\n",
    "train = df.groupby('num_bbox', group_keys=False).apply(lambda x: x.sample(frac=0.8))\n",
    "test = df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "841a4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['image_path'] = f'{ROOT_DIR}/working/COTS/images/train/'+df.image_id+'.jpg'\n",
    "train['label_path'] = f'{ROOT_DIR}/working/COTS/labels/train/'+df.image_id+'.txt'\n",
    "test['image_path'] = f'{ROOT_DIR}/working/COTS/images/valid/'+df.image_id+'.jpg'\n",
    "test['label_path'] = f'{ROOT_DIR}/working/COTS/labels/valid/'+df.image_id+'.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9efc48",
   "metadata": {},
   "source": [
    "## copy images to train and test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "957a0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copy(row):\n",
    "    shutil.copyfile(row.old_image_path, row.image_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2781f8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17fd926f7db4eaaa1afb8aa74373a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _,row in tqdm(train.iterrows(), total=len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a96e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2756d50f5f954e5a84c4415588b1a9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy) (row) for _,row in tqdm(test.iterrows(), total=len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6242f2d2",
   "metadata": {},
   "source": [
    "## create annotation txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c10e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco2yolo(bboxes, height=720, width=1280):\n",
    "    # normalising\n",
    "    bboxes[..., 0::2] /= width\n",
    "    bboxes[..., 1::2] /= height\n",
    "    \n",
    "    #conversion (xmin, ymin) -> (xmid, ymid)\n",
    "    bboxes[..., 0:2] += bboxes[..., 2:4]/2\n",
    "    return bboxes\n",
    "\n",
    "def annot2str(data):\n",
    "    data = data.astype(str)\n",
    "    string = '\\n'.join([' '.join(annot) for annot in data])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f6fa0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa7b15410aa49888ddcea4087a203e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(train.shape[0])):\n",
    "    row = train.iloc[i]\n",
    "    image_height = 720\n",
    "    image_width = 1280\n",
    "    bboxes_coco = np.array(row.bboxes).astype(np.float32).copy()\n",
    "    labels = np.array([0]*row.num_bbox)[..., None].astype(str)\n",
    "    ## Create Yolo Annotations txt file\n",
    "    with open(row.label_path, 'w') as f:\n",
    "        if row.num_bbox < 1:\n",
    "            annot = ''\n",
    "            f.write(annot)\n",
    "            continue\n",
    "        bboxes_yolo = coco2yolo(bboxes_coco, image_height, image_width)\n",
    "        #print(bboxes_yolo)\n",
    "        annots = np.concatenate([labels, bboxes_yolo], axis=1)\n",
    "        #print(annots)\n",
    "        string = annot2str(annots)\n",
    "        #print(string)\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e6887c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb206556d29a4126b8ae8851f56648a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(test.shape[0])):\n",
    "    row = test.iloc[i]\n",
    "    image_height = 720\n",
    "    image_width = 1280\n",
    "    bboxes_coco = np.array(row.bboxes).astype(np.float32).copy()\n",
    "    labels = np.array([0]*row.num_bbox)[..., None].astype(str)\n",
    "    ## Create Yolo Annotations txt file\n",
    "    with open(row.label_path, 'w') as f:\n",
    "        if row.num_bbox < 1:\n",
    "            annot = ''\n",
    "            f.write(annot)\n",
    "            continue\n",
    "        bboxes_yolo = coco2yolo(bboxes_coco, image_height, image_width)\n",
    "        annots = np.concatenate([labels, bboxes_yolo], axis=1)\n",
    "        string = annot2str(annots)\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07fab843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /kaggle/working/yolov5/data/data.yaml: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(F'{ROOT_DIR}/working/train.txt', 'w') as f:\n",
    "    for path in glob('/kaggle/working/train/*'):\n",
    "        f.write(path+'\\n')\n",
    "            \n",
    "with open(F'{ROOT_DIR}/working/val.txt', 'w') as f:\n",
    "    for path in glob('/kaggle/working/val/*'):\n",
    "        f.write(path+'\\n')\n",
    "\n",
    "data = dict(\n",
    "    train = '../../COTS/images/train',\n",
    "    val = '../../COTS/images/valid',\n",
    "    \n",
    "    nc    = 1, # number of classes\n",
    "    names =  ['cots'] # classes\n",
    "    )\n",
    "\n",
    "with open(F'{ROOT_DIR}/working/yolov5/data/dataset.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n",
    "\n",
    "%cat /kaggle/working/yolov5/data/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "450d347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arpitsinghrulania/kaggle/working/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64842909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=16, imgsz=1280, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=kaggle-Reef, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "YOLOv5 ðŸš€ v6.2-239-gf33718f Python-3.9.12 torch-1.13.0+cu117 CUDA:0 (GeForce RTX 2080 Ti, 11016MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir kaggle-Reef', view at http://localhost:6006/\n",
      "2022-11-15 02:35:21.924264: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 02:35:22.055276: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 02:35:22.516952: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-11-15 02:35:22.517039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-11-15 02:35:22.517048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:03<00:00, 4.18MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/home/arpitsinghrulania/kaggle/working/COTS/labels/train' image\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/arpitsinghrulania/kaggle/working/COTS/images/train/0-9470.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0021]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/arpitsinghrulania/kaggle/working/COTS/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/arpitsinghrulania/kaggle/working/COTS/labels/valid' images \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/arpitsinghrulania/kaggle/working/COTS/labels/valid.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.16 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to kaggle-Reef/exp2/labels.jpg... \n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mkaggle-Reef/exp2\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0%|          | 0/246 [00:00<?, ?it/s]                                         \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arpitsinghrulania/kaggle/working/yolov5/train.py\", line 630, in <module>\n",
      "    main(opt)\n",
      "  File \"/home/arpitsinghrulania/kaggle/working/yolov5/train.py\", line 524, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/home/arpitsinghrulania/kaggle/working/yolov5/train.py\", line 306, in train\n",
      "    pred = model(imgs)  # forward\n",
      "  File \"/home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/arpitsinghrulania/kaggle/working/yolov5/models/yolo.py\", line 209, in forward\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "  File \"/home/arpitsinghrulania/kaggle/working/yolov5/models/yolo.py\", line 121, in _forward_once\n",
      "    x = m(x)  # run\n",
      "  File \"/home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/arpitsinghrulania/kaggle/working/yolov5/models/common.py\", line 57, in forward\n",
      "    return self.act(self.bn(self.conv(x)))\n",
      "  File \"/home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/arpitsinghrulania/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --img {IMG_SIZE} \\\n",
    "                 --batch {BATCH_SIZE} \\\n",
    "                 --epochs {EPOCHS} \\\n",
    "                 --data dataset.yaml \\\n",
    "                 --weights yolov5s.pt \\\n",
    "                 --project kaggle-Reef "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "620c4346fd8a1dcff834024e7ffd348b316544d03ce35c7c54b7c359c59af504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
